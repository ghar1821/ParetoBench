{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing algorithms with ParetoBench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will compare the performance of 2 algorithms, ChronoClust and FlowSOM on 2 mass cytometry datasets, Levine13 and Levine32, using 4 metrics, F1-score, ARI, Accuracy, and V-measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have not downloaded and installed ParetoBench, please go to our github page and download ParetoBench (https://github.com/ghar1821/ParetoBench).\n",
    "Unzip it to a folder and run the setup.py file (python3 setup.py install).\n",
    "\n",
    "Then run the next cell to import ParetoBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ParetoBench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying details for the comparison study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main function which will compare the algorithms performance is call \"compare\".\n",
    "For this to work, there are a few things you need to first specify: \n",
    "\n",
    "* Metrics used for comparison\n",
    "* Dataset names\n",
    "* Algorithm names\n",
    "* Location where the quality of clustering solutions are stored\n",
    "* The column denoting the unique identifier of the clustering result\n",
    "\n",
    "The first 3 are rather self explanatory. \n",
    "The 4th one is the directory containing all the csv files storing the quality of algorithm's clusterings.\n",
    "The last one is the name of the column in each clustering solutions' quality result file denoting the unique identifier for that solution. \n",
    "Let's explore this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the folder containing the data is stored under ~/Documents/phd/code/ParetoBench/examples/data.\n",
    "Let's inspect its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '~/Documents/phd/code/ParetoBench/examples/data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bc0fff2dba24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'~/Documents/phd/code/ParetoBench/examples/data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '~/Documents/phd/code/ParetoBench/examples/data'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_dir = '//Documents/phd/code/ParetoBench/examples/data'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we have 2 folders, each named after the algorithm we want to compare. \n",
    "Let's look into what's inside flowsom folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('{}/flowsom'.format(data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 2 datasets, and thus we need to have 2 files in the flowsom ***and chronoclust*** folder.\n",
    "Each file must contain the quality of flowsom's clustering solutions for a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dat = pd.read_csv('{}/flowsom/Levine32_scores.csv'.format(data_dir))\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the score file looks like. \n",
    "Each row represents a clustering solution produced by flowsom.\n",
    "Each column (f1, accuracy, ari, v_measure) indicates the metrics used to evaluate the clustering solutions.\n",
    "Param column denotes the parameter id of that clustering solution, and seed is the seed used to generate that clustering solution.\n",
    "The seed column here is unique to flowsom. \n",
    "If we look at the chronoclust's score, we won't have this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv('{}/chronoclust/Levine32_scores.csv'.format(data_dir))\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead we only have the 4 metrics and the parameter column. \n",
    "This is the bare minimum required for ParetoBench to function.\n",
    "If you require the parameter value for each solution, by all means add them in (just as the case with flowsom above). \n",
    "ParetoBench will simply ignore it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all of that hopefully make sense.\n",
    "Let's define the parameters for ParetoBench."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParetoBench parameter definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['accuracy', 'ari', 'f1', 'v_measure']\n",
    "datasets = ['Levine13', 'Levine32']\n",
    "algorithms = ['chronoclust', 'flowsom']\n",
    "datadir = data_dir\n",
    "savedir = '/Users/givanna/Documents/phd/code/ParetoBench/examples/pareto_comparison'\n",
    "param_id_col = 'param'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Very important!***\n",
    "\n",
    "The metrics name must match the columns in the score files.\n",
    "\n",
    "The datasets name must match the score filename (before the _scores.csv)\n",
    "\n",
    "The algorithms name must match the folder name storing the score files.\n",
    "\n",
    "savedir is basically the location where ParetoBench will output the result. \n",
    "\n",
    "param_id_col denotes the name of the column which uniquely identify your result. This shall allow you to identify (post-running) which result is in which front.\n",
    "In this example, I'm just using straight numbering. \n",
    "You can share same numbering/values across different result files, but not within the same result file.\n",
    "\n",
    "Now let's run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run ParetoBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ParetoBench.compare(\n",
    "    metrics = metrics,\n",
    "    datasets = datasets,\n",
    "    algorithms = algorithms,\n",
    "    datadir = datadir,\n",
    "    savedir = savedir,\n",
    "    param_id_col = param_id_col\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You won't see anything returned, which is normal as ParetoBench store the results as csv files in savedir.\n",
    "Let's look at the result now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ParetoBench results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(savedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 4 different types of csv files here:\n",
    "\n",
    "1. front_positions_XXX.csv: this shall show you the front positions (normalised or not) of each clustering solution. If XXX is all_datasets, then it's basically the concatenation of front positions for each dataset. Do note that this file is important if you are looking at the normalised front positions as it's normalised based on ***all datasets***, not ***not per dataset!***\n",
    "2. ks_XXX.csv: comparison of distribution of the normalised front positions using KS test.\n",
    "3. proportion_solutions_per_front_XXX.csv: the number of solutions (and thus proportion) contributed by each algorithm to each front position.\n",
    "4. summary_XXX.csv: handy single value summary data highlighting the proportion solutions from each algorithm residing on Pareto front, top 10% and 33% of the fronts.\n",
    "\n",
    "Let's look at each result in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Front_positions_XXX.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv(\"{}/front_positions_Levine13.csv\".format(savedir))\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can see which solution reside in which front position, which parameter index it is, and what are the metric scores (and the negation used to compute the fronts).\n",
    "You can also see the normalised front position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ks_XXX.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv(\"{}/ks_Levine13.csv\".format(savedir))\n",
    "dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is comparison of the normalised fronts distribution of the algorithms. \n",
    "You can see for this dataset, the difference is statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### proportion_solutions_per_front_XXX.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv(\"{}/proportion_solutions_per_front_Levine13.csv\".format(savedir))\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of solutions in each front for each algorithm. \n",
    "Proportion column presents the count as the proportion of number of solutions contributed by the algorithm for that dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### summary_XXX.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv(\"{}/summary_Levine13.csv\".format(savedir))\n",
    "dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains single summary value showing the proportion of solutions residing on Pareto front, top 10% and 33% of the fronts.\n",
    "These can be used to quickly infer which algorithm is superior.\n",
    "The one contributes the most solutions to Pareto front performed better while that with higher proportion of solutions in top 10% and/or 33% is less vulnerable to parameter variations.\n",
    "\n",
    "The top 10% and 33% can be changed to any value by specifying them (as an array of x/100) as fronts_limits argument for the compare function.\n",
    "For example, if you want 20% and 50% respectively, pass [0.2, 0.5] as fronts_limits argument to compare method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the manuscript, in addition to extracting the number of solutions in the pareto front for each dataset and algorithm, we draw up CDF plots and swarm plots to interpret the results.\n",
    "\n",
    "Now, the plots in the manuscript are drawn using ggplot in R, just because well it's prettier..\n",
    "You can use the same script if you want. \n",
    "They're stored under plots folder of ParetoBench directory in github.\n",
    "\n",
    "You can of course draw them using Seaborn, but I don't like how it looks. \n",
    "\n",
    "Refer to separate notebook on how to create those pretty visualisations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
